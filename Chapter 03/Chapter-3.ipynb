{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 구조적 스트리밍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame = spark.read.format('csv')\\\n",
    ".option('header', 'true')\\\n",
    ".option('inferSchema', 'true')\\\n",
    ".load(\"./ybigta_de/input/sparkData/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame.createOrReplaceTempView('retail_data')\n",
    "staticSchema = staticDataFrame.schema #스키마도 함께 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과가 교재랑 다른데, 코드와 데이터가 완전 똑같은데 왜 다른지 도저히 모르겠습니다.\n",
    "(교재 gitHub에서 복붙해서 실행도 해봤는데 똑같았습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------+\n",
      "|CustomerId|              window|  sum(total_cost)|\n",
      "+----------+--------------------+-----------------+\n",
      "|   16057.0|[2011-12-05 00:00...|            -37.6|\n",
      "|   14126.0|[2011-11-29 00:00...|643.6300000000001|\n",
      "|   13500.0|[2011-11-16 00:00...|497.9700000000001|\n",
      "|   17160.0|[2011-11-08 00:00...|516.8499999999999|\n",
      "|   15608.0|[2011-11-11 00:00...|            122.4|\n",
      "+----------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 윈도우 함수: 집계 시 시계열 컬럼을 기준으로 각 날짜에 대한 전체 데이터를 가지는 윈도우 구성\n",
    "\n",
    "from pyspark.sql.functions import window, col\n",
    "\n",
    "# 특정 고객(customerID)이 대량으로 구매하는 영업 시간\n",
    "staticDataFrame\\\n",
    "  .selectExpr(\n",
    "    \"CustomerId\",\n",
    "    \"(UnitPrice * Quantity) as total_cost\",\n",
    "    \"InvoiceDate\")\\\n",
    "  .groupBy(\n",
    "    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    "  .sum(\"total_cost\")\\\n",
    "  .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍\n",
    "# read -> readStream\n",
    "# maxFilesPerTrigger\n",
    "\n",
    "streamingDataFrame = spark.readStream\\\n",
    "    .schema(staticSchema)\\\n",
    "    .option('maxFilesPerTrigger', 1)\\\n",
    "    .format('csv')\\\n",
    "    .option('header', 'true')\\\n",
    "    .load('./ybigta_de/input/sparkData/retail-data/by-day/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingDataFrame.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 총 판매 금액 계산\n",
    "\n",
    "purchaseByCustomerPerHour = streamingDataFrame\\\n",
    "    .selectExpr(\n",
    "        \"CustomerID\",\n",
    "        \"(UnitPrice * Quantity) as total_cost\",\n",
    "        \"InvoiceDate\")\\\n",
    "    .groupBy(\n",
    "        col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    "    .sum('total_cost')\n",
    "# 지연연산이므로 출력 x 액션 호출해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f6fa008cb50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchaseByCustomerPerHour.writeStream\\\n",
    "    .format('memory')\\\n",
    "    .queryName('customer_purchases')\\\n",
    "    .outputMode('complete')\\\n",
    "    .start()\n",
    "# 인메모리 테이블에 저장, 인메모리 테이블명, 모든 카운트 수행 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   18102.0|[2010-12-07 00:00...|          25920.37|\n",
      "|      null|[2010-12-06 00:00...|23395.099999999904|\n",
      "|      null|[2010-12-03 00:00...| 23021.99999999999|\n",
      "|      null|[2010-12-01 00:00...|12584.299999999988|\n",
      "|   15061.0|[2010-12-02 00:00...| 9407.339999999998|\n",
      "|   13777.0|[2010-12-01 00:00...|           6585.16|\n",
      "|   17850.0|[2010-12-02 00:00...|3891.8699999999985|\n",
      "|   16029.0|[2010-12-01 00:00...|           3702.12|\n",
      "|   17381.0|[2010-12-06 00:00...|           2567.64|\n",
      "|   13089.0|[2010-12-06 00:00...|2496.2200000000003|\n",
      "|   16210.0|[2010-12-01 00:00...|2474.7399999999993|\n",
      "|   13081.0|[2010-12-03 00:00...|           2366.78|\n",
      "|   16210.0|[2010-12-06 00:00...|2263.7999999999993|\n",
      "|   17450.0|[2010-12-07 00:00...|           2028.84|\n",
      "|   15061.0|[2010-12-07 00:00...|           2022.16|\n",
      "|   16754.0|[2010-12-02 00:00...|            2002.4|\n",
      "|   12433.0|[2010-12-01 00:00...|1919.1400000000008|\n",
      "|   15299.0|[2010-12-02 00:00...|1835.0100000000002|\n",
      "|   14733.0|[2010-12-07 00:00...|           1830.02|\n",
      "|   17511.0|[2010-12-01 00:00...|           1825.74|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM customer_purchases ORDER BY `sum(total_cost)` DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f6fa009ec10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 처리 결과 콘솔 출력\n",
    "\n",
    "purchaseByCustomerPerHour.writeStream\\\n",
    "    .format('console')\\\n",
    "    .queryName('customer_purchases_2')\\\n",
    "    .outputMode('complete')\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 머신러닝과 고급 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staticDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "\n",
    "preppedDataFrame = staticDataFrame\\\n",
    "    .na.fill(0)\\\n",
    "    .withColumn('day_of_week', date_format(col('InvoiceDate'), 'EEEE'))\\\n",
    "    .coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train과 test로 분리\n",
    "\n",
    "trainDataFrame = preppedDataFrame\\\n",
    "    .where(\"InvoiceDate < '2011-07-01'\")\n",
    "testDataFrame = preppedDataFrame\\\n",
    "    .where(\"InvoiceDate >= '2011-07-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296006"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 액션 호출하여 데이터 분리\n",
    "\n",
    "trainDataFrame.count()\n",
    "testDataFrame.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark MLlib은 일반적인 트랜스포메이션을 자동화하는 다양한 트랜스포메이션 제공\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# 요일을 수치형을 반환 (토-6, 월-1)\n",
    "indexer = StringIndexer()\\\n",
    "    .setInputCol('day_of_week')\\\n",
    "    .setOutputCol('day_of_week_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요일을 대소비교 가능한 숫자로 표현하는 것은 문제가 될 수 있어 OneHotEncoder로 불리언 값으로 표현\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\\\n",
    "    .setInputCol('day_of_week_index')\\\n",
    "    .setOutputCol('day_of_week_encoded')\n",
    "\n",
    "# 벡터 타입을 구성할 컬럼 중 하나로 사용\n",
    "# 스파크의 모든 머신러닝 알고리즘은 수치형 벡터 타입을 입력으로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler()\\\n",
    "    .setInputCols(['UnitPrice', 'Quantity' ,'day_of_week_encoded'])\\\n",
    "    .setOutputCol('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  나중에 입력값으로 들어올 데이터가 같은 프로세스를 거쳐 변환되도록 파이프라인 설정\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transformationPipeline = Pipeline()\\\n",
    "    .setStages([indexer, encoder, vectorAssembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer fit\n",
    "\n",
    "fittedPipeline = transformationPipeline.fit(trainDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTraining = fittedPipeline.transform(trainDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string, day_of_week: string, day_of_week_index: double, day_of_week_encoded: vector, features: vector]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 동일한 트랜스포메이션을 계속 반복할 수 없으므로 모델에 일부 하이퍼파라미터 튜닝값 제공\n",
    "# 캐싱을 사용해 중간 변환된 데이터셋의 복사본을 메모리에 저장해 전체 파이프라인을 재실행하는것보다 빠르게 접근\n",
    "\n",
    "transformedTraining.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 위해 클래스 임포트 & 인스턴스 생성\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans()\\\n",
    "  .setK(20)\\\n",
    "  .setSeed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "kmModel = kmeans.fit(transformedTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97324473.83355041"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 비용 계산\n",
    "kmModel.computeCost(transformedTraining)\n",
    "\n",
    "transformedTest = fittedPipeline.transform(testDataFrame)\n",
    "kmModel.computeCost(transformedTraining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 저수준 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자를 병렬화해 RDD 생성\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark.sparkContex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
